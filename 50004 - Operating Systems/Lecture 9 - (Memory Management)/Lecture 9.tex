\documentclass{report}
    \title{50004 - Operating Systems - Lecture 9}
    \author{Oliver Killane}
    \date{19/11/21}

\input{../50004 common.tex}

\begin{document}
    \maketitle
    \lectlink{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=f30d25c9-662e-4191-a155-ade200eb5f9e}

    \section*{Memory Management}
        \begin{itemize}
            \bullpara{Von Neumann Architecture}{
                \\ Data and instructions are stored in the same memory (as opposed to Havard Architecture), every instruction requires a memory access.}
            \bullpara{Memory Allocation}{
                \\ The OS must be able to allocate memory to programs (i.e malloc/calloc/realloc).
                \\
                \\ This needs to be fast \& efficient as programs will allocate data on the heap often, and rerquire memory allocated to load.
            }
            \bullpara{Memory Protection}{
                \\ OS must prevent programs for certain memory. This ensures that processes can run in isolation 
                as another process cannot corrupt their memory and cause a crash, likewise with the kernel and use processes. 
                \\
                \\This is also important for security as processes may want to hide data (e.g passwords, keys etc) from other untrusted processes.
            }
        \end{itemize}

        \subsection*{Memory Hierarchy}
            \centerimage{width=0.6\textwidth}{memory hierarchy.png}
            Note that the cost (monetary) per size of memory increases from with speed (registers most expensive, disk least).
            \\
            \begin{itemize}
                \bullpara{Registers}{
                    \\ Accessible in a single clock, can store a limited size (usually 64 bits at most) and are very expensive.
                    \\
                    \\ For example \keyword{x86\_64} architecture uses fully-general purpose 16 registers
                }
                \bullpara{Cache}{
                    \\ L3 is typically shared between cores. L2 \& L1 per core.
                    \\
                    \\ Caches can have differing levels of associativity, and hold values of recently accessed memory addresses.
                }
            \end{itemize}
        
        \subsection*{Basic Concepts}
            \compitem{
                \item Memory Allocation
                \item Swapping
                \item Paging (Virtual Memory)
                \item Page Replacement Algorithms
                \item Working Set Model
            }

        \subsection*{Logical and Physical Addresses}
            Memory management binds the logical address space to a physical address.
            \begin{itemize}
                \bullpara{Logical Address}{ Generated by the CPU and the address used by processes.}
                \bullpara{Physical Address}{Address used by memory unit, refering to a location in physical system memory.}
            \end{itemize}
            The use of logical addresses can change based on the \keyword{address-binding scheme}.
            \sidenote{Address Binding Scheme}{
                Determines when and how logical addresses are bound to physcial addresses, there are 3 such schemes:
                \begin{itemize}
                    \bullpara{Compile Time}{
                        \\ When the program is compiled, the address binding is set.
                        \\
                        \\ The compiler interacts with the OS's memory management.
                    }
                    \bullpara{Load Time}{
                        \\ When a program is loaded into memory, the addresses are bound.
                        \\
                        \\ Binding is done by the OS memory manager (e.g application loader).
                    }
                    \bullpara{Execution Time/Dynamic Address Binding}{
                        \\ Memory binding postponed until execution starts, with binding done by the processor.
                        \\
                        \\ This is the most common type used by practically all modern operating systems.
                    }
                \end{itemize}
            }
            In Compile \& Load time address-binding, the addresses used by the program are physical addresses. (Program binary is updated to contain these).
            \\
            \\ In Execution time binding logical addresses are used, and the CPU translates these to physical addresses at runtime.
        
        \subsection*{Memory Management Unit}
            A hardware device to map logical to physical addresses.
            \centerimage{width=0.7\textwidth}{mmu.png}
            The \keyword{base register} holds the smallest physical address, the \keyword{limit register} holds the highest logical address. This way the \keyword{MMU} can ensure:
            \[base \leq \text{translated address} < base + limit\]
            By ensuring processes can only access certain contiguous sections of memory, memory of the kernel and other processes is protected.
            \centerimage{width=\textwidth}{memory layout.png}
        
        \subsection*{Multiple Partition Allocation}
            \begin{itemize}
                \bullpara{Hole}{A contiguous section of unallocated memory.}
                \bullpara{Creation/Destruction}{
                    \\ When a new process is started, the OS allocates the required memory from a sufficiently large hole.
                    \\
                    \\ When a process is terminated, its memory is deallocated, and now free to be allocated to another process.
                }
                \bullpara{OS Requirements}{
                    \\ The \keyword{OS} must keep track of which partitions have been allocated and which are free (holes).
                }
            \end{itemize}
            \centerimage{width=0.8\textwidth}{memory holes.png}
        
        \subsection*{Dynamic Storage Allocation}
            When allocating request for a certain size from the available list of holes and their sizers, held by the OS.
            \centerimage{scale = 0.4}{hole allocation start.png}
            \begin{itemize}
                \bullpara{First Fit}{ Allocate in the first hole that is big enough. (Fast \& Simple)
                    \centerimage{scale = 0.4}{first fit allocation.png}
                    }
                \bullpara{Best-Fit}{ Allocate the smallest hole that is large enough.
                    \compitem{
                        \item Unless the list of holes is ordered, we must traver the whole list to decide which hole to use.
                        \item Produces smallest leftover memory after allocation.
                    }
                    \centerimage{scale = 0.4}{best fit allocation.png}
                }
                \bullpara{Worst-Fit}{ Allocate largest hole.
                    \compitem{
                        \item Search entire list (unless ordered)
                        \item The largest possible leftover produced.
                    }
                    \centerimage{scale = 0.4}{worst fit allocation.png}
                }
            \end{itemize}
        
        \subsection*{Fragmentation}
            \begin{itemize}
                \bullpara{External Fragmentation}{Enough memory avaiulable, but not hole large enough.
                    \\ When we have a large number of very small holes. We can fix this by \keyword{compaction}:
                    \compitem{
                        \item Shuffle memory contents to place all free memory together in one block.
                        \item Can result in I/O bottlenecks, as requires a very large amount of copying.
                    }
                    }
                \bullpara{Internal Fragmentation}{Allocated memory larger than the requested.
                    \\ e.g Partition allocated, but program does not make use of all the partition. 
                    \\ Wasted memory results in total memory available being low. }
            \end{itemize}
        
        \subsection*{Swapping}
            The number of processes is limited by the amount of available main memory. (Note: only running processes need to be in main memory)
            \\ 
            \\ We can supplement main memory with \keyword{swap space} (can be a partition or file) on secondary storage (e.g HDD or SSD). However we must be aware than transfer times are long, 
            and we must bring any process that is scheduled back into main memory.
            \centerimage{width=0.6\textwidth}{swapping.png}
    
    \section*{Virtual Memory with Paging}
        Virtual memory is an abstraction to separate logical memory from physical memory.
        \compitem{
            \item Not all of an executing process needs to be in memory for execution.
            \item Logical address space can be much larger than the physical address space.
            \item Address spaces can be shared between several processes.
            \item Process creation can become more efficient.
            \item If page size is fixed, external fragmentation is impossible.
        }
        \subsection*{Virtual Address Space}
            \centerimage{width=0.8\textwidth}{virtual address space.png}
        
        \begin{itemize}
            \bullpara{Frames}{
                \\ Fixed-size blocks of physical memory. The \keyword{OS} must keep track of all free frames.
            }
            \bullpara{Pages}{
                \\ A block the same size as a frame, in logical memory (effectively a logical frame).
            }
            \bullpara{Program Start}{
                \\ For a program of size $n$ frames.
                \begin{enumerate}
                    \item Find $n$ free frames, load the program into that memory.
                    \item Create a page table for the process, mapping logical pages to physical frames.
                \end{enumerate}
            }
            \bullpara{Program Termination}{ Destroy the page table, freeing all associated frames. }
        \end{itemize}
        \centerimage{width=0.8\textwidth}{page table.png}
        The advantages of this approach are:
        \compitem{
            \item Less internal fragmentation (If a page is not yet used, it is not allocated in memory).
            \item Less external fragmentation (When allocating alot of memory, split into pages and distribute across physical memory).
            \item Contiguous logical addresses can be physically non-contiguous.
        }
        \subsubsection*{Page Size}
            \begin{minipage}[t]{0.4\textwidth}
                \centerline{\textbf{Small Page Size}}
                \compitem{
                    \item Less internal fragmentation.
                    \item Potentially less memory to swap.
                    \item Larger Page table.
                }
                Best for efficient memory usage.
            \end{minipage}
            \hfill
            \begin{minipage}[t]{0.4\textwidth}
                \centerline{\textbf{Large Page Size}}
                \compitem{
                    \item More internal fragmentation
                    \item Smaller Page table.
                }
                Best for low address lookup overhead.
            \end{minipage}
        \subsubsection*{Context Switch}
            When switching process, the OS must locate the page table of the new process. Set the base register to point to the table in memory, and clear the invalidated cache from the TLB.
            \sidenote{TLB}{
                \keyword{Translation Lookaside Buffer} is used to cache the mappings of \keyword{Virtual} to \keyword{Physical} addresses. 
                \\
                \\ When a virtual address is used \& its corresponding physical address found, it is added to the cache so that if the virtual address is used again the physical address does not need to be recomputed.
            }
    
        \subsection*{Address Translation}
            Each memory address is split into a \keyword{page number}(p) and a \keyword{page offset}(d)
            \begin{itemize}
                \bullpara{Page Number}{
                    \\ The index of the page, used to get the base-address of the corresponding frame from the page table.}
                \bullpara{Page Offet}{
                    \\ The offset through the page/frame, which is combined with the base-address to get the physical address.}
            \end{itemize}
            For logical address space $2^m$ with page size $2^n$:
            \centerimage{width=\textwidth}{address translation.png}
            For example:
            \\ A 16-bit (big endian) system. 1KByte page size. Each page entry's LSB is the valid bit, and second LSB is modified (dorty) bit.
            Page table:
            \[\begin{matrix}
                0      & 1      & 2      & 3     & 4 \\
                0x2C00 & 0x0403 & 0xCC01 & 0x000 & 0x7C01 \\
            \end{matrix}\]
            Find the physical memory addresses of the following virtual addresses:
            \[\begin{matrix}
                0xB85 & 0x1420 & 0x1000 & 0xC9A \\
            \end{matrix}\]

            \begin{center}
                \begin{tabular}{p{6cm} c c c c}
                   Get addresses & 0xB85 & 0x1420 & 0x1000 & 0xC9A \\
                   & $\downarrow$ & $\downarrow$ & $\downarrow$ & $\downarrow$  \\
                   Divide address by 1024 to get page \# & 2 & 5 & 4 & 3 \\
                   & $\downarrow$ & $\downarrow$ & $\downarrow$ & $\downarrow$  \\
                   Get Page Table Entry & 0xCC01 & PAGE FAULT (past end) & 0x7C01 & 0x000 \\
                   & $\downarrow$ & $\downarrow$ & $\downarrow$ & $\downarrow$  \\
                   Check Valid \& Dirty Bits & VALID & N/A & VALID & PAGE FAULT (invalid) \\
                   & $\downarrow$ & $\downarrow$ & $\downarrow$ & $\downarrow$  \\
                   Divide entry by 1024 to get frame & 0x33 & N/A & 0x1F \\
                   & $\downarrow$ & $\downarrow$ & $\downarrow$ & $\downarrow$  \\
                   Add frame onto front of offset & 0xCF85 & N/A & 0x7C00 & N/A \\
                \end{tabular}
            \end{center}
    
        \subsection*{Memory Protection}
            We associate protection bits with each pages.
            \\
            \\ We associate a valid-invalid bit with each entry in the page table.
            \begin{itemize}
                \bullpara{Valid}{ Associated page is in process' logical address space.}
                \bullpara{Invalid}{ Page is missing!
                    \compitem{
                        \item Page not in process' logical address space (page fault).
                        \item Must load page from swap space (on disk) (see demand paging).
                        \item Incorrect access of some kind.
                    }
                }
            \end{itemize}
            \sidenote{Demand Paging}{
                \keyword{Demand paging} is a method of memory management where pages in the swap space on the disk are only loaded into main memory when an access attempt is made.
                \\
                \\ This is opposed to \keyword{anticipatory paging} (the norm) where program's pages are loaded from the swap when it starts running, in anticipation of them being accessed.
            }

        \subsection*{Page Table Implementation}
            \compitem{
                \item \keyword{Page Table} is kept in main memory. 
                \item The \keyword{Page-Table base register} (\keyword{PTBR}) points to the page table's start
                \item The \keyword{page-table length register} (\keyword{PTLR}) indicates size.
            }
            However using the page table for every memory access is slow (requires access to page table in memory), so caching is used.
            \\
            \\ Note that for kernel mode operations, a bit is set in the page table
            \sidenote{Associative Memory}{
                Also called \keyword{CAM} (Content addressable memory), it is a special type of memory optimised for search (making use of parallelism).
                \\
                \\ Generally much more expensive than \keyword{RAM} and can only be used for a fixed memory allocation format (cannot search for different data types after creating the silicon).
            }
            \centerimage{width=\textwidth}{page table and TLB.png}
            \subsubsection*{Translation Look-aside Buffer}
                An \keyword{associative memory} cache that stores translations of virtual pages to physical frames.
                \\
                \sidenote{Address-Space IDs}{
                    Some \keyword{TLB}s store \keyword{address-space ids} (\keyword{ASID}s) in entires. As a result the TLB does not have to be fully wiped when context switching,
                    as it can recognise that old entries are invalid using the associated \keyword{ASID}.
                    \\
                    \\ This potentially improves performance as entries may survive a short context switch, and no cycles are wasted in removal.
                    \\
                    \\ The \keyword{MIPS} and \keyword{ARM} architecture's TLB uses this approach with 8-Bits ASIDs, and many other architectures have added this feature.
                }
                The \keyword{reach} of the TLB is the number of addresses that can be cached, for larger page sizes, the reach is larger.
            \subsubsection*{Kernel Page access in System Calls}
                When a process makes a system call, and the system call handler runs in privileged mode int that process, it may need to access kernel pages.
                \\
                \\ To ensure safe access, the page table will have a \keyword{supervisor} bit. If set for a translation, it means that when in kernel mode, the page can be accessed.
                \\
                \\ For example with page table:
                \[\begin{matrix}
                    Page & Frame & Valid & Supervisor \\
                    0    & 3     & 1     & 0 \\
                    1    & 5     & 1     & 0 \\
                    2    & 14    & 1     & 1 \\
                \end{matrix}\]
                A process cannot access page $2$ without a segfault, however if in kernel mode following a system call, that page can be accessed.
                \\
                \\ An alternative implementation would use separate page tables for user space and kernel mode execution.
        
        \subsection*{Effective Access Time}
            Given that:
            \compitem{
                \item ($\omega$) Memory cycle time.
                \item ($\epsilon$) Associative Lookup time.
                \item ($\alpha$) Hit rate for TLB (also referred to as Associative Registers/Memory) (larger tlb cache $\rightarrow$ higher hit rate)
            }
            \[\text{Effective Access Time (EAT)} = (\epsilon + \omega) \alpha + (\epsilon + 2 \times \omega)(1 - \alpha)\]
            (If hit, one memory lookup, else 2.)
        \\
        \subsection*{Page Table Types}
            Break up the logical address space into multiple page tables:
            \subsubsection*{Multi-Level/Hierarchical Page Table}
                Page table size can be given by:
                \[\text{page table size} = \cfrac{\text{Address Space Size}}{\text{Page Size}} \times \text{Entry Size}\]
                As a result for very large address spaces, with small page sizes, the page table can become large.
                \\
                \\ This is an issue as:
                \compitem{
                    \item If page table is larger than a frame - added complexity.
                    \item Page table uses lots of memory, even if a process does not access any/many pages.
                }
                To resolve this we can use multi-level page tables - trees containing page tables. 
                \\
                \\ For $n$ levels the page number is partitioned into $n$ sections. To access a page:
                \begin{enumerate}
                    \bullpara{Get Outermost Page table}{Typically in a single frame.}
                    \bullpara{For each partition of the page number}{
                        \begin{enumerate}
                            \item Get index from partition of page number.
                            \item Get entry at index in the current frame (a page table).
                            \item If entry is invalid, empty or dirty, then page fault.
                            \item Else get the frame from the entry. Set this as the current frame.
                        \end{enumerate}
                    }
                    \bullpara{Get physical address}{ In the current frame, add the page offset.}
                \end{enumerate}
                
                An example of a two-level page table is below.
                \centerimage{width=\textwidth}{two level page table.png}
                
                The disadvantage of this system is there are more memory accesses to allow a process to access memory (in a TLB miss).
                \\
                \\ For example a system with memory access time of $100ns$ and TLB access of $20ns$.
                \begin{center}
                    \begin{tabular}{l c c}
                        \textbf{Hit Rate} & \textbf{Single Level} & \textbf{4-Level} \\
                        $80\%$ & $0.8 (20 + 100) + 0.2(20 + 2 \times 100) = 140ns$ & $0.8 (20 + 100) + 0.2 (20 + 5 \times 100) = 200ns$ \\
                        $98\%$ & $0.98 (20 + 100) + 0.02(20 + 2 \times 100) = 122ns$ & $0.98 (20 + 100) + 0.02 (20 + 5 \times 100) = 128ns$ \\
                    \end{tabular}
                \end{center}
                As unpaged memory access requires only $100ns$ we can see the performance of paged memory access gets closer as hit rate increases, even at large paging levels.


            \subsubsection*{Hashed Page Table}
                A hash table to map page numbers to frame numbers. (A normal page table is effectively a Hashed Page table with identity as the hash function.)
                \\
                \\ By using more complex hash functions, we can reduce the size of the page table, at the expense of conflicts. Where we can use a linked list to resolve conflicts.
                \centerimage{width=0.8\textwidth}{hashed page table.png}
                The diagram above uses \keyword{bucket based hashmap} using linked lists as buckets. Here entries are hashed and added to an appropriate bucket (with their unhashed key). 
                When hash collisions occur, buckets are filled with more than one entry. When retrieving from hashmap, the hash of the key (for the query) is used to find the correct bucket, and then equality of the keys associated with each entry are used to find the correct entry for the given key.
            
            \subsubsection*{Inverted Page Table}
                A page table containing an entry for every page frame of memory, containing:
                \compitem{
                    \item Virtual Address of page stored at location.
                    \item Information on the owning process (e.g process ID).
                }
                The means only a single page table is used. And it becomes very fast to find the process \& page number associated with any given frame (hence the name \textit{inverted}).
                \centerimage{width=0.8\textwidth}{inverted page table.png}
                Compared with the standard page table implementation:
                \compitem{
                    \item Decreases memory needed to store page table.
                    \item Increased time to search table (e.g must filter/search through enties with other process' IDs).
                    \item Can use a hash table to limit linear search to a few entries.
                }
    \section*{Shared Memory}
        Processes can access shared memory by having pages in two processes point to the same frame.
        \\
        \\ Once the pages are setup to achieve this, the kernel does not need to be involved.
        \centerimage{width=0.6\textwidth}{shared memory.png}
        This is useful for IPC, as well as other uses such as sharing libraries.
        \\
        \\ Comparison with pipes:
        \compitem{
            \item Higher performance (less kernel intervention, and data can be kept in space - less copying)
            \item Bi-Directional communicqation possible (otherwise would require two pipes).
            \item Less useful for unidirectional communication as kernel provides synchronisation for pipes.
        }
        For example \keyword{System V}'s API (note more modern systems use \keyword{mmap}):
        \codelist{C}{system V shared memory.c}
        \sidenote{System V}{
            One of the first commercial \keyword{Unix} Operating systems originally developed by \keyword{AT\&T} and released in 1983.
            \\
            \\ Was a competitor/rival to \keyword{BSD} with much cross pollination fo features occurring between the two. It also had several distributions by other companies such as Oracle's Solaris OS.
            \centerimage{width=\textwidth}{unix history.png}
        }
        
\end{document}
