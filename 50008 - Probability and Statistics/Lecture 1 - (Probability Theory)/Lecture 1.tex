\documentclass{report}
    \title{50008 - Probability and Statistics - Lecture 1}
    \author{Oliver Killane}
    \date{11/01/22}
\input{../50008 common.tex}
\begin{document}
\maketitle

\section*{Elementary Probability Theory}
\lectlink{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=0804d63e-685c-49c5-9d1b-ae1800b0ba1d}

Probability theory is a mathematical formalism to describe and quantify uncertainty.
\\
\\ Uses of probability include examples such as:
\compitem {
	\item Finding distribution of runtimes \& memory usage for software.
	\item Response times for database queries.
	\item Failure rate of components in a datacenter.
}

\subsection*{Sample Spaces and Events}
\lectlink{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=b3d2d5a9-2820-498d-88b3-ae1800b0baa7}
\termdef{Sample Space}{
	The set of all possible outcomes of a random experiment. The set is usually denoted with set notation, and can be finite, countably or uncountably infinite.
	\\
	\\ For example:
	\begin{center}
		\begin{tabular}{l l}
			\textbf{Experiment}  & \textbf{Sample Space}                                                \\
			\hline
			Coin Toss            & $S = \{Heads, Tails\}$                                               \\
			6-Sided Dice Roll    & $S = \{1,2,3,4,5,6\}$                                                \\
			2 Coin Tosses        & $S = \{(H,H), (H,T), (T,H), (T,T)\}$                                 \\
			Choice of Odd number & $S = \{ x \in \mathbb{N} | \exists y \in \mathbb{N}.[2y + 1 = x] \}$
		\end{tabular}
	\end{center}
}
\termdef{Event}{
	Any subset of the sample space $E \subseteq S$ (a set of possible outcomes).
	\compitem{
		\bullpara{null event($\emptyset$)}{Empty event, can be used for impossible events.}
		\bullpara{universal event ($S$)}{Event contains entire sample space and is therefore certain.}
		\bullpara{elementary events}{Singleton subsets of the sample space (contain one element).}
	}
	For example:
	\begin{center}
		\begin{tabular}{l l l}
			\textbf{Event}             & \textbf{Set of Event} & \textbf{Sample Space}                \\
			\hline
			6-Sided Dice Rolls 1       & $E = \{1\}$           & $S = \{1,2,3,4,5,6\}$                \\
			6-Sided Dice Rolls Even    & $E = \{2,4,6\}$       & $S = \{1,2,3,4,5,6\}$                \\
			6-Sided Dice Rolls 7       & $E = \emptyset$       & $S = \{1,2,3,4,5,6\}$                \\
			2 Coin toss get 2 Tails    & $E = \{(T,T)\}$       & $S = \{(H,H), (H,T), (T,H), (T,T)\}$ \\
			Random Natural Number is 4 & $E = \{4\}$           & $S = \mathbb{N}$                     \\
		\end{tabular}
	\end{center}
}
\centerimage{width=0.7\textwidth}{experiment.png}
\compitem{
	\item If we perform a random experiment with outcome $S* \in S$. If $s* \in E$, then event $E$ has occurred.
	\item If $E$ has not occurred ($s* \not\in E$) then $s* \in \overline{E}$.
	\item The set $\{s*\}$ is an elementary event.
	\item Null event $\emptyset$ never occurs, the universal event $S$ always occurs.
}
\subsubsection*{Set Operations on Events}
\begin{itemize}
	\bullpara{Union / Or}{
		\[\bigcup_iE_i = \{s \in S | \exists i .[s \in E_i] \} \]
		Occurs if at least one of the events $E_i$ has occurred (has union of event sets).
		\\
		\\ If 4 is rolled on a 6-sided dice, then union of (is 3) and (is 4) occurred.
	}
	\bullpara{Intersection / And}{
		\[\bigcap_iE_i = \{s \in S | \forall i. [s \in E_i]\}\]
		Occurs if all the events $E_i$ occur.
		\\
		\\ If 4 is rolled on a 6-sided dice, the intersection of (is even) and (is 4) occurred.
	}
	\bullpara{Mutual Exlusion}{
		\[E_1 \cap E_2 = \emptyset\]
		If sets are disjoint, then they are mutually exclusive (cannot occur simultaneously).
		\\
		\\ For a 6-sided dice the events (is 4) and (is 6) arte mutually exclusive.
	}
\end{itemize}

\subsection*{Probability}
\lectlink{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=1d8ad6f9-ad83-4625-a63a-ae1800b0bad5}
When determining the probability of every subset $E \subseteq S$ occurring:
\compitem{
	\bullpara{$S$ is Finite}{Can easily assign probabilites.}
	\bullpara{$S$ is countable}{Can assign probabilites.}
	\bullpara{$S$ is uncountably infinite}{
		\\ Can initially assign some collection of subsets probabilities, but it then becomes impossible to define probabilities on reamining subsets.
		\\
		\\ Cannot make probabilities sum to $1$ with reasonably axioms.
	}
}
For this reason when defining a probability function on sample space $S$, we must define the collection of subsets we will measure.
\\
\\ The subsets are referred to as $\mathcal{F}$ and must be:
\compenum{
	\item nonempty ($S \in \mathcal{F}$)
	\item closed under complements $E \in \mathcal{F} \Rightarrow \overline{E} \in \mathcal{F}$
	\item closed under countable union $E_1, E_2, \dots \in \mathcal{F} \Rightarrow \bigcup_iE_i \in \mathcal{F}$
}
A collection of sets is known as $\sigma$-algebra.
\termdef{Probability Measure}{
	A function $P: \mathcal{F} \to [0,1]$ on the pair $(S,\mathcal{F})$ such that:
	\begin{center}
		\begin{tabular}{l l}
			Axiom 1. & $\forall E \in \mathcal{F}.[0 \leq P(E) \leq 1]$                                                                   \\
			Axiom 2. & $P(S) = 1$                                                                                                         \\
			Axiom 3. & Countably additive, for \textbf{disjoint} sets $E_1, E_2, \dots \in \mathcal{F}$: $P(\bigcup_iE_i) = \sum_iP(E_i)$
		\end{tabular}
	\end{center}
	$P(E)$ provides the probability (between 0 and 1 inclusive) that a given event occurs.
}
From the axioms satisfied by a \keyword{probability measure} we can derive that:
\compenum{
	\item $P(\overline{E}) = 1 - P(E)$
	\item $P(\emptyset) = 0$
	\item For any events $E_1$ and $E_2$: $P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)$
}

\subsection*{Interpretations of Probability}
\lectlink{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=2ef8d19d-1e83-4019-8e44-ae1800b0e0a1}
\subsubsection*{Classical Interpretation}

Given $S$ is finite and the \keyword{elementary events} are equally likely:
\[P(E) = \cfrac{|E|}{|S|}\]
We can also extend this \keyword{uniform probability distribution} to infinite spaces by considering measures such as area, mass or volume.
\centerimage{width=0.5\textwidth}{uniform area.png}

\subsection*{Frequentist Interpretation}
Through repeated observations of identical random experiments in which $E$ can occur, the proportion of experiments where $E$ occurs tends towards the probability of $E$.
\\
\\ At an infinite number experiments, the proportion of occurrences of $E$ is equal to $P(E)$.
\sidenote{Central Limit Theorem}{
	This can also be considered in terms of \keyword{central limit theorem}, where the greater the sample size taken from some distribution (with defined mean $\mu$), the closer the mean of the sample to the distribution's mean. (more readings results in less variance in the sample means as they converge on the distribution's mean)
}

\subsection*{Subjective Interpretation}
Probability is the degree of belief held by an individual.
\\
\\ For example if gambling: \begin{tabular}{l l}
	Option 1: $E$ occurs win $£1$, $\overline{E}$ occurs win $£0$ \\
	Option 2: Regardless of outcome get $£P(E)$                   \\
\end{tabular}.
\\
\\ Either outcome, the gambler receives $£P(E)$. The value of $P(E)$ is the value for which the individual is indifferent about the choice between option 1 or 2. It is the \keyword{individuals probability} of event $E$ occurring.


\subsection*{Joint Events and Conditional Probability}
\lectlink{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=291512d9-06e4-4795-a936-ae1800b1063f}
We commonly need to consider \keyword{Join Events} (where two events occur at the same time).
\termdef{Independent Events}{
Two events are independent if the occurence of one does not affect the other. Given $E_1$ and $E_2$ are independent:
\[E_1 \ and \ E_2 \ independent \Leftrightarrow P(E_1 \ occurrs \ and \ E_2 \ occurs) = P(E_1) \times P(E_2)\]
More generally, the set of events $\{E_1, E_2, \dots\}$ are independent if for any finite subset $\{E_{i_1}, E_{i_2}, \dots, E_{i_n}\}$:
\[p(\bigcap_{j=1}^{n}E_{i_j}) = \prod_{j=1}^n P(E_{i_j}) \]
If $E_1$ and $E_2$ are independent, then so are $\overline{E_1}$ and $E_2$.
\\
\\For example with a coin toss, subsequent coin tosses do not effect the next coin toss's probability of heads.
}

We can show that if $E_1$ and $E_2$ are independent, so are $\overline{E_1}$ and $E_2$:
\begin{proof}
	\proofstep{1}{$F = (E_1 \cap E_2) \cup (\overline{E_1} \cap E_2)$}{By set operations}
	\proofstep{2}{$P(E_2) = P(E_1 \cap E_2) + p(\overline{E_1} \cap E_2)$}{As \stepno{1} was a disjoint union, Axiom 3}
	\proofstep{3}{$P(\overline{E_1} \cap E_2) = P(E_2) - P(E_1 \cap E_2)$}{}
	\proofstep{4}{$P(\overline{E_1} \cap E_2) = P(E_2) - P(E_1) \times P(E_2)$}{}
	\proofstep{5}{$P(\overline{E_1} \cap E_2) = P(E_2) \times (1 - P(E_1)$}{}
	\proofstep{6}{$P(\overline{E_1} \cap E_2) = P(E_2) \times P(\overline{E_1})$}{By $P(\overline{E}) = 1 - P(E)$}
\end{proof}
We can show that $P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)$:
\begin{proof}
	\proofstep{1}{$E_1 \cup E_2 = E_1 \cup (E_2 \cap \overline{E_1})$}{From set theory}
	\proofstep{2}{$P(E_1 \cup E_2) = P(E_1 \cup (E_2 \cap \overline{E_1}))$}{By Axiom 3}
	\proofstep{3}{$P(E_1 \cup E_2) = P(E_1) + P(E_2 \cap \overline{E_1})$}{}
	\proofstep{4}{$P(E_2 \cap \overline{E_1}) = P(E_2) - P(E_1 \cap E_2)$}{By \stepno{3} of the previous proof and as $E_1$ and $E_2$ are independent}
\end{proof}
\subsubsection*{For Example: Coin and 6-Sided Dice}
We can construct a \keyword{Probability Table}:
\begin{center}
	\begin{tabular}{c c c c c c c c c}
		\setlength{\tabcolsep}{3em}
		                           &                & \multicolumn{6}{c}{Dice} & \multirow{2}{*}{Totals}                                                                                          \\
		                           &                & 1                        & 2                       & 3               & 4               & 5               & 6                                \\
		\multirow{2}{*}{Coin}      & H              & $\sfrac{1}{12}$          & $\sfrac{1}{12}$         & $\sfrac{1}{12}$ & $\sfrac{1}{12}$ & $\sfrac{1}{12}$ & $\sfrac{1}{12}$ & $\sfrac{1}{2}$ \\
		                           & T              & $\sfrac{1}{12}$          & $\sfrac{1}{12}$         & $\sfrac{1}{12}$ & $\sfrac{1}{12}$ & $\sfrac{1}{12}$ & $\sfrac{1}{12}$ & $\sfrac{1}{2}$ \\
		\multicolumn{2}{c}{Totals} & $\sfrac{1}{6}$ & $\sfrac{1}{6}$           & $\sfrac{1}{6}$          & $\sfrac{1}{6}$  & $\sfrac{1}{6}$  & $\sfrac{1}{6}$  &                                  \\
	\end{tabular}
\end{center}
We can determine the probability of any event by summing the probabilities of elementary events represented by cells in the table.
\\
\\ $P({H})$ is called a \keyword{marginal probability}, as it the probability of one event occurring irrespective of the other (the dice in this case).
\\
\\ $P({(H,3)})$ is called a \keyword{joint probability} as it involves both events (dice roll and the coin toss).

\subsubsection*{For Example: Coin and 2 6-Sided Dice}
A crooked die (called a top) has the same faces on either side.
\\
\\ We flip the coin, then if it is heads we use the normal die, else we use the top.
\begin{center}
	\begin{tabular}{c c c c c c c c c}
		\setlength{\tabcolsep}{3em}
		                           &                & \multicolumn{6}{c}{Dice} & \multirow{2}{*}{Totals}                                                                                          \\
		                           &                & 1                        & 2                       & 3               & 4               & 5               & 6               &                \\
		\multirow{2}{*}{Coin}      & H              & $\sfrac{1}{12}$          & $\sfrac{1}{12}$         & $\sfrac{1}{12}$ & $\sfrac{1}{12}$ & $\sfrac{1}{12}$ & $\sfrac{1}{12}$ & $\sfrac{1}{2}$ \\
		                           & T              & $\sfrac{1}{6}$           & $0$                     & $\sfrac{1}{6}$  & $0$             & $\sfrac{1}{6}$  & $0$             & $\sfrac{1}{2}$ \\
		\multicolumn{2}{c}{Totals} & $\sfrac{1}{4}$ & $\sfrac{1}{12}$          & $\sfrac{1}{4}$          & $\sfrac{1}{12}$ & $\sfrac{1}{4}$  & $\sfrac{1}{12}$ &                                  \\
	\end{tabular}
\end{center}
We can now see that $P({(H,3)}) \neq P({H}) \times P({3})$ and hence they are dependent, as the dice roll depends on the coin toss.

\subsection*{Conditional Probability}
\lectlink{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=71a90c30-38fd-4a9f-b02e-ae1800b0ba59}
For two events $E$ and $F$ in \keyword{sample space} $S$, where $P(F) \neq 0$:
\[P(E|F) = \cfrac{P(E \cap F)}{P(F)}\]
Probability of $E$ given $F$ is the probability of both occurring over the probability of $F$.
\sidenote{Independence}{
	If $E$ and $F$ are independent:
	\[P(E|F) = \cfrac{P(E \cap F)}{P(F)} = \cfrac{P(E) \times P(F)}{P(F)} = P(E)\]
}
\termdef{Conditional Independence}{
	$P(\bullet | F)$ defines a probability measure obeying the axioms of probability on set $F$ (When have just reduced $S$ to $F$).
	\\
	\\ Three events $E_1, E_2, F$ are conditionally independent if and only if:
	\[P(E_1 \cap E_2|F) = P(E_1|F) \times P(E_2|F)\]
}


\subsubsection*{Example: Rolling a Dice}
What is the probability the dice rolls a $3$ given the dice rolls an odd number?
\[P(\{3\}|\{1,3,5\}) = \cfrac{P(\{3\} \cap \{1,3,5\})}{P(\{1,3,5\})} = \cfrac{P(\{3\})}{P(\{1,3,5\})} = \cfrac{\sfrac{1}{6}}{\sfrac{1}{2}} = \cfrac{1}{3}\]

\subsubsection*{Example: Rolling two Dice}
Throw a die from each hand. What is the probability the die thrown from the left is larger than the die thrown from the right.
\\
\\ The sample space is:
\[S = \begin{matrix}
		\{(1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(2,1),(2,2),(2,3),(2,4),(2,5),(2,6),(3,1),(3,2),(3,3),(3,4),(3,5),(3,6), \\
		(4,1),(4,2),(4,3),(4,4),(4,5),(4,6),(5,1),(5,2),(5,3),(5,4),(5,5),(5,6),(6,1),(6,2),(6,3),(6,4),(6,5),(6,6)\}  \\
	\end{matrix}\]
We want the event such that the left value of the pair is larger.
\\
\\ For value $1$ there are $0$ possible, for $2$ there is $1$ and so on.
\[(1:0),(2:1),(3:2),(4:3),(5:4),(6:5)\]
Hence there are $0 + 1 + 2 + 3 + 4 + 5 = 15$ possible pairs with the left larger than the right.
\[P(E) = \cfrac{15}{36} = \cfrac{5}{12}\]
However if we know the left or right die, we can determine a new probability. For example if we know the left die is $4$ then we know there are $6$ pairs with the left as $4$, and $3$ of those pairs have a smaller right.
\[P(E|{4}) = \cfrac{3}{6} = \cfrac{1}{2}\]

\termdef{Bayes Theorem}{
	For two events $E$ and $F$ we have:
	\[P(E \cap F) = P(F) \times P(E|F) = P(F) \times \cfrac{P(E \cap F)}{P(F)} = P(E) \times P(F|E) = P(E) \times \cfrac{P(E \cap F)}{P(E)}\]
	Hence we can deduce:
	\[P(E|F) = \cfrac{P(E) \times P(F|E)}{P(F)}\]
	\centerimage{width=0.6\textwidth}{bayes theorem.png}
}

\termdef{Partition Rule}{
	Given a set of events $\{F_1, F_2, \dots\}$ which forms a partition of $S$ (disjoint sets that contain all of $F$).
	\\
	\\ For any event $E \subseteq S$:
	\[P(E) = \sum_iP(E|F_i) \times P(F_i)\]
	\centerimage{width=0.8\textwidth}{partition rule.png}
	Proof:
	\begin{proof}
		\proofstep{1}{$E = E \cap S = E \cap \bigcup_iF_i = \bigcup_i(E \cap F_i)$}{By set theory and disjointness of partitions.}
		\proofstep{2}{$P(E) = P(\bigcup_i(E \cap F_i))$}{}
		\proofstep{3}{$P(E) = \sum_i{P(E \cap F_i)}$}{By axiom 3 and disjointness of partitions.}
		\proofstep{4}{$P(E) = \sum_iP(E|F_i) \times P(F_i)$}{}
	\end{proof}
}

\termdef{Law of Total Probability}{
Given some event $E$ and events $\{F_1, F_2, \dots\}$:
\[P(E) = \sum_i{P(E \cap F_i)}\]
For example the 6-Sided dice, $E = {H}$ and $F = [\{1\},\{2\},\{3\},\{4\},\{5\},\{6\}]$, the marginal probability is the same as the sum of all cells in row $H$.
}

Using complement as a partition we can deduce that:
\[P(E) = P(E \cap F) + P(E \cap \overline{F})\]
\[P(E) = P(E|F) \times P(F) + P(E|\overline{F}) \times P(\overline{F})\]

\subsection*{Terminology Recap}
\begin{itemize}
	\bullpara{Conditional Probabilities}{Of the form $P(E | F)$.}
	\bullpara{Joint Probabilities}{Of the form $P(E \cap F)$.}
	\bullpara{Marginal Probabilities}{Of the form $P(E)$.}
\end{itemize}












\end{document}
